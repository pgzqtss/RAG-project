import os
import json
from tqdm import tqdm
from langchain_openai import ChatOpenAI
import dotenv
import textwrap
from sentence_transformers import SentenceTransformer, util
import torch

# âœ… Load environment variables
dotenv.load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… Define directories
INDEX_NAME = "fuck"
USER_PAPERS_DIR = "backend/user_uploads"
OUTPUT_DIR = "backend/processed_texts"
OUTPUT_PATH = os.path.join(OUTPUT_DIR, "systematic_review.json")

# âœ… Ensure directories exist
os.makedirs(OUTPUT_DIR, exist_ok=True)

# âœ… Initialize LLM model
model = ChatOpenAI(api_key=OPENAI_API_KEY,
                   model='gpt-3.5-turbo',
                   temperature=0)

def generate_background_section(results, query, model, chunk_size=5):
    """å•ç‹¬ç”Ÿæˆ Background ç« èŠ‚"""
    section_title = "Background"
    section_prompt = textwrap.dedent(f"""
        {section_title}
                                        
        Generate the {section_title} section for the systematic review using the relevant information from the retrieved documents. 
        Integrate findings directly from the provided sources and ensure all claims are supported by the retrieved evidence.

        ğŸ”¹ Required Content:
            1.	Introduction to the Topic:
                â€¢	Introduce the research topic and highlight its significance using insights from the most relevant retrieved documents.
                â€¢	Define essential concepts based on the retrieved material, ensuring consistency with the latest research.
                â€¢	Discuss the broader relevance of the topic, drawing on real-world examples or applications mentioned in the retrieved sources.
            2.	Summary of Existing Literature:
                â€¢	Summarize key findings from the retrieved documents, focusing on significant trends, major discoveries, and methodologies.
                â€¢	Critically assess the strengths, limitations, and relevance of the studies retrieved.
                â€¢	Address inconsistencies or conflicting evidence found in the sources, offering possible explanations or highlighting gaps.
            3.	Justification for This Systematic Review:
                â€¢	Clearly explain why this review is necessary based on the gaps or unresolved issues identified in the retrieved documents.
                â€¢	Describe how this review will address limitations or contribute new insights beyond what the retrieved studies have already established.
                â€¢	Define the research question clearly. Use frameworks like PICO (Population, Intervention, Comparison, Outcome) only if supported by the retrieved material.

        ğŸ”¹ Writing Style and Source Integration Guidelines:

        âœ… Use a formal, academic tone appropriate for scholarly work
        âœ… Prioritize evidence from the retrieved sources to support all claims
        âœ… Avoid including information not supported by the retrieved evidence.
        âœ… Address conflicting information from different sources, providing analysis or explanation
        âœ… Focus on depth and relevanceâ€”highlight the most pertinent findings
        âœ… Adapt the depth of each section based on the relevance and strength of the retrieved evidence
    """).strip()
    return generate_section(results, query, model, section_title, section_prompt, chunk_size)


def generate_methods_section(results, query, model, chunk_size=5):
    """Generate the Methods section for a Systematic Review following PRISMA guidelines."""
    
    section_title = "Methods"
    section_prompt = textwrap.dedent(f"""
        {section_title}
                                     
        Generate the {section_title} section for the Systematic Review using relevant information retrieved through a RAG system. 
        The output should follow the structure of a systematic review, emphasizing clarity, logical flow, and evidence-based reasoning.

        ğŸ”¹ Required Content:
            1.	Study Design:
                â€¢	Specify this is a Systematic Review.
                â€¢	State that the methodology aligns with PRISMA guidelines where applicable, focusing on transparency and reproducibility.
            2.	Evidence Synthesis:
                â€¢	Summarize the key themes and findings from the retrieved research.
                â€¢	Highlight any consistent patterns, agreements, or notable contrasts in the evidence.
            3.	Inclusion Rationale:
                â€¢	Emphasize the relevance of the information retrieved to the query.
                â€¢	Clearly explain why certain insights were prioritized (e.g. relevance to the research question, recent developments).
            4.	Strengths and Limitations of Retrieved Information:
                â€¢	Discuss the general strengths of the body of information retrieved.
                â€¢	Note any potential gaps, such as limited coverage or conflicting information.
            5.	Key Insights Summary:
                â€¢	Present major findings, outcomes, or conclusions derived from the available data.
                â€¢	Focus on clear, concise summaries without requiring detailed statistical breakdowns.

        ğŸ”¹ Writing Style:

        âœ… Clear, structured, and evidence-informed
        âœ… No citations requiredâ€”focus on summarizing key points
        âœ… Present insights logically, highlighting relevance to the research query  
    """).strip()

    return generate_section(results, query, model, section_title, section_prompt, chunk_size)


def generate_results_section(results, query, model, chunk_size=5):
    """Generate the Results section for a Systematic Review."""

    section_title = "Results"
    section_prompt = textwrap.dedent(f"""
        {section_title}                             

        Generate the {section_title} section for the Systematic Review using relevant information retrieved through a RAG system. Structure the content logically and focus on summarizing the key findings from the retrieved information.

        ğŸ”¹ Required Content:
            1.	Relevant Information Summary:
                â€¢	Provide a general overview of the information retrieved that is relevant to the research query.
                â€¢	Focus on summarizing the main themes and findings present across the retrieved documents.
            2.	Study Types and Methodological Insights (If Available):
                â€¢	Briefly describe any study types or methodological details that appear in the retrieved information (e.g., experimental research, observational analysis).
                â€¢	Only include this information if it is explicitly mentioned in the retrieved content.
            3.	Key Findings and Themes:
                â€¢	Present a clear, concise summary of major findings from the retrieved content.
                â€¢	Highlight recurring themes or notable differences, without inferring patterns from incomplete information.
            4.	General Observations and Gaps:
                â€¢	Mention any apparent gaps in the retrieved information, such as missing data on certain outcomes or study populations.
                â€¢	Avoid making assumptionsâ€”only report what is supported by the retrieved content.

        ğŸ”¹ Writing Style:

        âœ… Focused on clear, unbiased summaries
        âœ… Only present information directly supported by retrieved content
        âœ… Flexible structure, prioritizing relevance over strict formatting
    """).strip()

    return generate_section(results, query, model, section_title, section_prompt, chunk_size)


def generate_discussion_section(results, query, model, chunk_size=5):
    """Generate the Discussion section for a Systematic Review."""

    section_title = "Discussion"
    section_prompt = textwrap.dedent(f"""
        {section_title}

        Generate the {section_title} section for the Systematic Review using relevant information retrieved through a RAG system. 
        Focus on summarizing the most relevant findings, clearly presenting insights while acknowledging any gaps or inconsistencies.

        ğŸ”¹ Required Content:
            1.	Summary of Main Findings:
                â€¢	Concisely summarize the key results from the retrieved content.
                â€¢	Highlight consistent themes and acknowledge any conflicting information without attempting to resolve discrepancies.
            2.	Interpretation of Findings:
                â€¢	Discuss potential explanations supported by the retrieved evidence.
                â€¢	Where applicable, note any preliminary findings or well-established conclusions relevant to the research topic.
            3.	Strengths and Limitations of Retrieved Information:
                â€¢	Describe strengths of the available information (e.g., relevance, breadth of coverage).
                â€¢	Address limitations, including gaps, conflicting findings, or lack of depth in the retrieved content.
            4.	Potential Areas for Future Research:
                â€¢	Suggest general future research directions based on observed gaps or trends in the retrieved evidence.
                â€¢	Avoid proposing specific methodological recommendations unless explicitly supported by the retrieved information.

        ğŸ”¹ Additional Guidelines:

        âœ… Prioritize the most relevant and clearly supported findings
        âœ… Acknowledge conflicting evidence without resolving it
        âœ… Use cautious, evidence-based language (e.g., â€œmay suggest,â€ â€œpotentially indicatesâ€)
        âœ… Clearly state when information is incomplete or lacking depth 
    """).strip()

    return generate_section(results, query, model, section_title, section_prompt, chunk_size)


def generate_conclusion_section(results, query, model, chunk_size=5):
    """Generate the Conclusion section for a Systematic Review."""

    section_title = "Conclusion"
    section_prompt = textwrap.dedent(f"""
        {section_title}

        Generate the {section_title} section for a Systematic Review using the following structure and guidelines:

        ğŸ”¹ Required Content:

        1.  Concise Summary of Key Findings:
            â€¢   Summarize the most critical findings from the study, focusing on outcomes that are strongly supported by the retrieved evidence.
            â€¢   Ensure the summary is clear, concise, and directly tied to the data retrieved by the RAG system.
        2.  Clinical & Public Health Implications:
            â€¢   Highlight practical implications for healthcare practice or policy, based on the retrieved evidence.
            â€¢   Clearly state limitations and uncertainties to avoid overgeneralization.
            â€¢   Use cautious language (e.g., "may suggest," "could potentially") when discussing implications that are not definitively supported by the evidence.
        3.  Final Recommendations:
            â€¢   Propose specific, actionable next steps for research or practice, addressing gaps or limitations identified in the retrieved evidence.
            â€¢   Avoid generic statements like "more research is needed"â€”instead, specify the type of research or areas requiring further investigation.
            â€¢   Ensure recommendations are grounded in the findings and directly linked to the retrieved data.
        ğŸ”¹ Writing Style:

        âœ… Evidence-based and precise: Use only information retrieved by the RAG system. Do not introduce new ideas or unsupported claims.
        âœ… Structured and logical: Ensure a clear flow from summary to implications to recommendations.
        âœ… Cautious and balanced: Acknowledge limitations and avoid overstating results.
        âœ… Actionable and specific: Provide tailored recommendations that are practical and relevant to stakeholders.
    """).strip()

    return generate_section(results, query, model, section_title, section_prompt, chunk_size)



# Load BERT model for similarity-based filtering
bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# Define length constraints for different sections
section_length_limits = {
    "Background": 1200,  # 800-1200 words
    "Methods": 1500,  # 1000-1500 words
    "Results": 2500,  # 1500-2500 words
    "Discussion": 3000,  # 2000-3000 words
    "Conclusion": 600,  # 300-600 words
}

def generate_section(results, query, model, section_title, section_prompt, 
                     previous_sections=None, chunk_size=30, similarity_threshold=0.8):
    """
    Generates a **Systematic Review** section with:
    - **Context awareness** (previously generated sections as input)
    - **De-duplication** (removes redundant content using BERT-based similarity checking)
    - **Smooth transitions** (AI is explicitly instructed to generate transition sentences)
    - **Flexible length control** (section-specific length limits)
    
    Parameters:
    - results: Research data in chunks
    - query: Current research topic
    - model: AI model for text generation
    - section_title: The title of the section being generated
    - section_prompt: Writing instructions for the section
    - previous_sections: Previously generated sections (to enhance continuity)
    - chunk_size: Number of data chunks to use for context
    - similarity_threshold: Sentence similarity threshold for de-duplication
    """

    seen_sentences = set()  # Store unique sentences

    # ğŸ”¥ Ensure `previous_sections` is always a list
    if not isinstance(previous_sections, list):  
        previous_sections = []  # å¼ºåˆ¶è½¬æ¢ä¸ºç©ºåˆ—è¡¨ï¼Œç¡®ä¿ä¸ä¼šå‡ºé”™

    # Retrieve the max length for this section
    max_length = section_length_limits.get(section_title, 1500)  # Default to 1500 words if unspecified

    # 1ï¸âƒ£ **Context Expansion** (Use both research data and previously generated sections)
    context_data = "\n\n".join(results[:chunk_size]) if results else ""

    # ğŸ”¥ é˜²æ­¢ `join()` æŠ¥é”™ï¼Œç¡®ä¿ previous_sections æ˜¯ list å¹¶ä¸”ä¸åŒ…å«éå­—ç¬¦ä¸²å…ƒç´ 
    previous_content = "\n\n".join(str(s) for s in previous_sections) if previous_sections else ""

    if previous_content:
        context_data = previous_content + "\n\n" + context_data  # Append previous sections for better flow

    # 2ï¸âƒ£ **Construct the Prompt**
    prompt = textwrap.dedent(f"""
    **Revised Systematic Review Writing Prompt**

    # ğŸ“š **Systematic Review Writing Task: {section_title}**

    You are an expert researcher conducting a **Systematic Review** following **PRISMA guidelines**. 
    Your task is to generate the **{section_title}** section in a **structured, evidence-based, and academic** manner.

    ## ğŸ” **Key Writing Guidelines**

    - Follow **PRISMA guidelines** for transparency & reproducibility.
    - Use **formal academic language** and ensure logical coherence with previous sections.
    - **Maintain logical flow** with clear transition sentences.
    - **Avoid redundancy**â€”summarize rather than repeat ideas.

    ## ğŸ“Œ **Context (Original Query, Previously Generated Sections & Research Data)**

    Below is the **original query** that sets the topic of the systematic review, followed by previously generated content and relevant research data:

    ```
    {query}
    ```

    ```
    {context_data}
    ```

    ## ğŸ›  **Instructions for Writing This Section**
    {section_prompt}

    - Use this data as a reference to ensure consistency.
    - If methodology or prior findings are referenced, align your section accordingly.

    ## ğŸ›  **Instructions for Writing This Section**

    ## ğŸ¯ **Output Constraints**

    - **Format:** Use clear headings & subheadings where necessary.
    - **Word Limit:** Aim for **{max_length} words**, prioritizing essential details.
    - **Clarity & Coherence:** Ensure smooth readability and logical consistency.

    ## ğŸ“ **Now, generate the full {section_title} section:**


    """)

    try:
        # 3ï¸âƒ£ **AI Generates the Text**
        response = model.invoke(prompt).content if model else None

        # ğŸ”¥ **Check if response is None**
        if response is None or not isinstance(response, str):
            raise ValueError(f"AI model did not return a valid response for {section_title}")

        response = response.strip()

        # ğŸ”¥ **Check if response is empty**
        if not response:
            raise ValueError(f"Generated response is empty for {section_title}")

        # Limit the text length based on the predefined constraints
        response = response[:max_length * 6]  # Approximate conversion: 1 word â‰ˆ 6 characters
        
        # 4ï¸âƒ£ **Deduplication Logic**
        generated_sentences = response.split("\n")
        unique_sentences = []

        for sentence in generated_sentences:
            if sentence.strip() and sentence not in seen_sentences:
                unique_sentences.append(sentence)
                seen_sentences.add(sentence)

        # 5ï¸âƒ£ **BERT-based Semantic Deduplication**
        filtered_sentences = []
        for sentence in unique_sentences:
            if not filtered_sentences:  # First sentence is always added
                filtered_sentences.append(sentence)
                continue
            
            # Compute similarity between new sentence and existing filtered ones
            embeddings1 = bert_model.encode([sentence], convert_to_tensor=True)
            embeddings2 = bert_model.encode(filtered_sentences, convert_to_tensor=True)
            similarity_scores = util.pytorch_cos_sim(embeddings1, embeddings2).mean().item()

            # Only add sentence if it is not too similar to previous ones
            if similarity_scores < similarity_threshold:
                filtered_sentences.append(sentence)

        final_section = "\n".join(filtered_sentences)  # Return the final cleaned section
        
        # âœ… **Append the generated section to previous_sections for future use**
        previous_sections.append(final_section)

        return final_section

    except ValueError as ve:
        print(f"âš ï¸ ValueError in {section_title}: {ve}")
        return ""
    except Exception as e:
        print(f"âš ï¸ Unexpected error in {section_title}: {e}")
        return ""




def generate_full_systematic_review(query, model):
    """é€æ­¥ç”Ÿæˆ Systematic Review çš„ä¸åŒç« èŠ‚ï¼Œæ¯ä¸ªç« èŠ‚ç‹¬ç«‹å¤„ç†"""

    paper_ids = get_all_paper_ids()  # âœ… è·å–æ‰€æœ‰å­˜å‚¨çš„è®ºæ–‡ ID
    systematic_review = {}

    print("ğŸ” ç”Ÿæˆ Background ç« èŠ‚...")
    background_results = search_pinecone_with_fallback(query, paper_ids=paper_ids, section="Background", top_k=50)
    systematic_review["Background"] = generate_background_section(background_results, query, model)

    print("ğŸ” ç”Ÿæˆ Methods ç« èŠ‚...")
    methods_results = search_pinecone_with_fallback(query, paper_ids=paper_ids, section="Methods", top_k=50)
    systematic_review["Methods"] = generate_methods_section(methods_results, query, model)

    print("ğŸ” ç”Ÿæˆ Results ç« èŠ‚...")
    results_results = search_pinecone_with_fallback(query, paper_ids=paper_ids, section="Results", top_k=50)
    systematic_review["Results"] = generate_results_section(results_results, query, model)

    print("ğŸ” ç”Ÿæˆ Discussion ç« èŠ‚...")
    discussion_results = search_pinecone_with_fallback(query, paper_ids=paper_ids, section="Discussion", top_k=50)
    systematic_review["Discussion"] = generate_discussion_section(discussion_results, query, model)

    print("ğŸ” ç”Ÿæˆ Conclusion ç« èŠ‚...")
    conclusion_results = search_pinecone_with_fallback(query, paper_ids=paper_ids, section="Conclusion", top_k=50)
    systematic_review["Conclusion"] = generate_conclusion_section(conclusion_results, query, model)

    return systematic_review


def process_and_store_pdfs():
    """Processes user-uploaded PDFs and stores embeddings in Pinecone."""
    
    user_files = [f for f in os.listdir(USER_PAPERS_DIR) if f.endswith(".pdf")]

    if not user_files:
        print("âš ï¸ No user-uploaded PDFs found in 'backend/user_uploads/'. Please upload files first.")
        return

    for pdf_file in tqdm(user_files, desc="Processing User Papers"):
        pdf_path = os.path.join(USER_PAPERS_DIR, pdf_file)
        paper_id = pdf_file.replace(".pdf", "")  # Use filename as namespace

        print(f"ğŸ“„ Extracting text from {pdf_file}...")
        text = pdf_to_text(pdf_path)
        classified_chunks = split_text_into_chunks(text)  # Now returns classified sections

        print(f"ğŸ“¦ Storing {len(classified_chunks)} chunks in Pinecone under '{paper_id}'...")
        store_chunks_in_pinecone(classified_chunks, paper_id=paper_id)

def main():
    """Pipeline for processing user PDFs and generating a systematic review."""

    # âœ… Step 1: Initialize Pinecone
    initialize_pinecone()

    # âœ… Step 2: Preload default research papers if missing
    # preload_research_papers()

    # âœ… Step 3: Process and store user PDFs
    process_and_store_pdfs()

    # âœ… Step 4: Generate systematic review
    query = "What are the key findings on COVID-19 vaccines?"
    print("ğŸ“– Generating final systematic review...")

    final_review = generate_full_systematic_review(query, model)

    # âœ… Step 5: Save results
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump({"review": final_review}, f, indent=4, ensure_ascii=False)

    print(f"âœ… Systematic review saved to {OUTPUT_PATH}")

if __name__ == "__main__":
    main()

###

import os
import pinecone
import dotenv

# Load API keys
dotenv.load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = "us-east-1"
INDEX_NAME = "fuck"
VECTOR_DIMENSION = 1536  # OpenAI Embeddings dimension

# New way to initialize Pinecone (v3+)
from pinecone import Pinecone, ServerlessSpec

# Initialize Pinecone client
pc = Pinecone(api_key=PINECONE_API_KEY)

def initialize_pinecone():
    """ Initializes Pinecone and creates an index if it doesn't exist. """
    
    # Check if index exists, if not create it
    if INDEX_NAME not in pc.list_indexes().names():
        print(f"Creating Pinecone index: {INDEX_NAME}...")
        pc.create_index(
            name=INDEX_NAME,
            dimension=VECTOR_DIMENSION,
            metric="cosine",
            spec=ServerlessSpec(
                cloud='aws',
                region='us-east-1'  
            )
        )
        print(f"Index '{INDEX_NAME}' created successfully!")
    else:
        print(f"Index '{INDEX_NAME}' already exists.")

if __name__ == "__main__":
    initialize_pinecone()

###

import fitz  # PyMuPDF for PDF processing
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
import os
import dotenv

# âœ… Load environment variables
dotenv.load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… Initialize OpenAI LLM
model = ChatOpenAI(api_key=OPENAI_API_KEY)

def pdf_to_text(pdf_path):
    """Extracts text from a PDF file."""
    doc = fitz.open(pdf_path)
    text = "\n".join([page.get_text("text") for page in doc])

    # Clean text: remove unnecessary whitespace & empty lines
    text = "\n".join([line.strip() for line in text.split("\n") if line.strip()])
    return text

def clean_text(text):
    """Cleans text by removing extra spaces, line breaks, and merging broken words."""
    text = text.replace('\n', ' ')
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[Â§â€ â€¡]', '', text)
    text = re.sub(r'\b(\w+)-\s+(\w+)\b', r'\1\2', text)  # Merge broken words
    return text.strip()

def classify_chunk_with_llm(chunk):
    """Classifies a text chunk into Background, Methods, Results, Discussion, or Conclusion using LLM."""
    prompt = f"""
    You are an AI assistant classifying research paper sections.
    Determine which section this text belongs to: Background, Methods, Results, Discussion, Conclusion.

    ---TEXT---
    {chunk}
    ------------------
    
    Output only the section name:
    """
    response = model.invoke(prompt).content.strip()
    valid_sections = {"Background", "Methods", "Results", "Discussion", "Conclusion"}
    return response if response in valid_sections else "Background"

def split_text_into_chunks(text, chunk_size=1500, overlap=300):
    """Splits text into smaller chunks while keeping sentence integrity and classifies sections."""
    text = clean_text(text)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap, 
                                                   separators=["\n\n", "\n", ".", "?", "!"])
    chunks = text_splitter.split_text(text)

    # Classify chunks into sections
    classified_chunks = [{"text": chunk, "section": classify_chunk_with_llm(chunk)} for chunk in chunks]
    
    return classified_chunks

if __name__ == "__main__":
    sample_pdf = "preloaded_papers/covid_vaccine_1.pdf"
    text = pdf_to_text(sample_pdf)
    classified_chunks = split_text_into_chunks(text)

    # Print first 5 classified chunks for testing
    for chunk in classified_chunks[:5]:
        print(f"ğŸ“„ Section: {chunk['section']}\nğŸ”¹ {chunk['text'][:500]}...\n")

###

import os
from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings

# âœ… Load API Keys
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# âœ… Connect to Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index("my-index")  # Update with your Pinecone index name

# âœ… Initialize OpenAI Embeddings
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

def search_pinecone(query, paper_id=None, section="Results", top_k=10):
    """Search for relevant text chunks in Pinecone based on Paper_ID and Section."""

    # Construct namespace based on whether a specific paper is requested
    if paper_id:
        namespace = f"SYSTEMATIC_REVIEW/{paper_id}/{section}"
    else:
        namespace = f"SYSTEMATIC_REVIEW/*/{section}"  # Search all papers' Results by default

    print(f"ğŸ” Searching Pinecone in namespace: '{namespace}' for query: '{query}'...")

    # Convert query to vector
    query_vector = embeddings.embed_query(query)

    # Query Pinecone index
    results = index.query(vector=query_vector, top_k=top_k, namespace=namespace, include_metadata=True)

    # Extract relevant text
    return [match["metadata"]["text"] for match in results["matches"]]


def get_all_paper_ids():
    """ä» Pinecone è·å–æ‰€æœ‰å­˜å‚¨çš„è®ºæ–‡ ID"""
    index_stats = index.describe_index_stats()
    paper_ids = set()

    for namespace in index_stats["namespaces"]:
        if namespace.startswith("SYSTEMATIC_REVIEW/"):
            parts = namespace.split("/")
            if len(parts) > 1:
                paper_ids.add(parts[1])

    return list(paper_ids)

def search_pinecone_with_fallback(query, paper_ids=None, section="Results", top_k=10):
    """åœ¨ Pinecone é‡Œæ ¹æ® Paper_ID å’Œ Section æœç´¢ç›¸å…³æ–‡æœ¬ç‰‡æ®µ"""

    query_vector = embeddings.embed_query(query)
    print(f"ğŸ” æŸ¥è¯¢å‘é‡ (å‰10ç»´): {query_vector[:10]}")  # ä»…æ‰“å°å‰ 10 ç»´ä»¥ä¾›è°ƒè¯•

    all_results = []

    if paper_ids is None:
        paper_ids = get_all_paper_ids()  # âœ… è‡ªåŠ¨è·å–æ‰€æœ‰ `Paper_ID`

    if not paper_ids:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å·²å­˜å‚¨çš„è®ºæ–‡ï¼Œæ— æ³•è¿›è¡ŒæŸ¥è¯¢ã€‚")
        return []

    print(f"ğŸ“„ å‘ç° {len(paper_ids)} ç¯‡å­˜å‚¨çš„è®ºæ–‡: {paper_ids}")

    # âœ… éå†æ‰€æœ‰ `Paper_ID`ï¼Œåˆ†åˆ«æŸ¥è¯¢
    for paper_id in paper_ids:
        namespace = f"SYSTEMATIC_REVIEW/{paper_id}/{section}"
        print(f"ğŸ” æ­£åœ¨ Pinecone æŸ¥è¯¢ namespace: '{namespace}'ï¼Œæœç´¢é—®é¢˜: '{query}'...")

        results = index.query(vector=query_vector, top_k=top_k, namespace=namespace, include_metadata=True)

        if results["matches"]:
            print(f"âœ… åœ¨ {namespace} ä¸­æ£€ç´¢åˆ° {len(results['matches'])} æ¡ç»“æœ")
            for match in results["matches"]:
                print(f"ğŸ“„ ç‰‡æ®µå†…å®¹: {match['metadata']['text'][:100]}...")
            all_results.extend([match["metadata"]["text"] for match in results["matches"]])
        else:
            print(f"âš ï¸ åœ¨ {namespace} æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")

    if not all_results:
        print("âš ï¸ ä»ç„¶æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœï¼Œè¯·æ£€æŸ¥ Pinecone æ•°æ®å­˜å‚¨æ˜¯å¦æ­£ç¡®")

    return all_results

##

import os
import dotenv
from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from process_pdf import pdf_to_text, split_text_into_chunks

# âœ… Load environment variables
dotenv.load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
INDEX_NAME = "fuck"

# âœ… Connect to Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)

# âœ… Ensure index exists
if INDEX_NAME not in pc.list_indexes().names():
    raise ValueError(f"âš ï¸ Index '{INDEX_NAME}' not found! Run `initialize_pinecone.py` first.")

index = pc.Index(INDEX_NAME)

# âœ… Initialize OpenAI Embeddings
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

# âœ… Initialize LLM model
llm_model = ChatOpenAI(api_key=OPENAI_API_KEY)

def classify_chunk_with_llm(chunk_text, model):
    """Classifies a text chunk into Background, Methods, Results, Discussion, or Conclusion using LLM."""
    prompt = f"""
    You are an AI assistant classifying research paper sections.
    The following text is an excerpt from a scientific paper.
    Determine whether it belongs to one of these sections:
    - Background
    - Methods
    - Results
    - Discussion
    - Conclusion

    If uncertain, choose the most relevant section.

    ---TEXT---
    {chunk_text}
    ------------------
    
    Output only the section name: 
    """

    try:
        response = model.invoke(prompt).content.strip()
        valid_sections = {"Background", "Methods", "Results", "Discussion", "Conclusion"}\
        
        classification = response if response in valid_sections else "Background"
        print(f"âœ… LLM classified chunk as '{classification}'")
        return classification
    except Exception as e:
        print(f"âš ï¸ LLM classification failed: {e}. Defaulting to 'Background'.")
        return "Background"

def get_text_embedding(text):
    """Convert text into vector embeddings using OpenAI embeddings."""
    if not isinstance(text, str):
        raise TypeError("âŒ get_text_embedding() received a non-string input.")

    return embeddings.embed_query(text)

def store_chunks_in_pinecone(text_chunks, paper_id):
    """Stores document chunks in Pinecone DB under Systematic Review namespaces."""

    global index  
    if INDEX_NAME not in pc.list_indexes().names():
        raise ValueError(f"âš ï¸ Index '{INDEX_NAME}' not found! Run `initialize_pinecone.py` first.")

    index = pc.Index(INDEX_NAME)

    # âœ… Get existing namespaces to avoid re-storing sections
    index_stats = index.describe_index_stats()
    existing_namespaces = index_stats.get("namespaces", {})

    # âœ… Instead of skipping the whole paper, only skip already stored sections
    stored_sections = {ns.split("/")[-1] for ns in existing_namespaces if ns.startswith(f"SYSTEMATIC_REVIEW/{paper_id}")}

    # âœ… Process each chunk separately
    for i, chunk in enumerate(text_chunks):
        if "text" not in chunk or not isinstance(chunk["text"], str):
            print(f"âš ï¸ Skipping invalid chunk {i} for '{paper_id}'.")
            continue

        # âœ… Call LLM for classification and ensure it's printed
        section = classify_chunk_with_llm(chunk["text"], llm_model)
        print(f"ğŸ” Chunk {i} classified as: {section}")  # âœ… Ensure we see LLM classification

        namespace = f"SYSTEMATIC_REVIEW/{paper_id}/{section}"

        # âœ… Skip storing chunks for sections that already exist
        if section in stored_sections:
            print(f"âš ï¸ Skipping chunk {i} (already stored in {namespace}).")
            continue

        vector = get_text_embedding(chunk["text"])

        # âœ… Explicitly store under correct namespace
        index.upsert([
            (
                f"{paper_id}-chunk-{i}",
                vector,
                {
                    "text": chunk["text"],
                    "source": paper_id,
                    "section": section
                }
            )
        ], namespace=namespace)

        print(f"âœ… Stored chunk {i} in Pinecone under '{namespace}'!")

    print(f"âœ… Successfully stored {len(text_chunks)} chunks in Pinecone under '{paper_id}'!")





def process_and_store_papers(directory="backend/papers"):
    """Processes all PDFs in the given directory and stores their embeddings in Pinecone."""
    
    if not os.path.exists(directory):
        print(f"âš ï¸ Directory '{directory}' not found.")
        return

    pdf_files = [f for f in os.listdir(directory) if f.endswith(".pdf")]

    if not pdf_files:
        print("âš ï¸ No PDF files found in the directory.")
        return

    for pdf_file in pdf_files:
        pdf_path = os.path.join(directory, pdf_file)
        paper_id = pdf_file.replace(".pdf", "")  # Use filename as paper ID

        print(f"ğŸ“„ Processing {pdf_file}...")

        text = pdf_to_text(pdf_path)

        if not text:
            print(f"âš ï¸ Skipping '{pdf_file}' (empty or unreadable).")
            continue

        classified_chunks = split_text_into_chunks(text)  # âœ… Returns classified dictionary chunks

        print(f"ğŸ”„ Storing {len(classified_chunks)} chunks in Pinecone under '{paper_id}'...")
        store_chunks_in_pinecone(classified_chunks, paper_id)  # âœ… Pass structured chunks

if __name__ == "__main__":
    process_and_store_papers()
